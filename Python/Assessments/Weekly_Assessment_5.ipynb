{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Algorithm Questions</h1>"
      ],
      "metadata": {
        "id": "dnWbyUtmfDNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does regularization (L1 and L2) help in preventing overfitting?\n",
        "\n",
        "* Regularization techniques, specifically L1 (Lasso) and L2 (Ridge) regularization, play a crucial role in preventing overfitting in machine learning models.\n",
        "\n",
        "* Overfitting occurs when a model learns the noise in the training data rather than the underlying patterns, leading to poor performance on unseen data.\n",
        "\n",
        "* Regularization addresses this issue by adding a penalty to the loss function that the model minimizes during training."
      ],
      "metadata": {
        "id": "tKJEWIOlenov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is feature scaling important in gradient descent?\n",
        "\n",
        "* Equal Contribution: Scaling ensures that all features contribute equally to the model, preventing any single feature from dominating due to its larger scale.\n",
        "\n",
        "* Faster Convergence: When features are on a similar scale, gradient descent can reach the optimal solution more quickly because the cost function becomes smoother.\n",
        "\n",
        "* Better Learning Rate: Properly scaled features allow for a higher learning rate, which speeds up training without risking divergence.\n",
        "\n",
        "* Consistent Units: Scaling helps when features are measured in different units (e.g., age vs. income), ensuring that no feature is unfairly weighted.\n",
        "\n",
        "* Improved Algorithm Performance: Many algorithms, especially those based on distances (like k-nearest neighbors), perform better with scaled data, leading to more accurate predictions."
      ],
      "metadata": {
        "id": "3oBlNIuYfBal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Problem Solving</h2>\n"
      ],
      "metadata": {
        "id": "Lfkv8HXzfpVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a dataset with missing values, how would you handle them before training an ML model?\n",
        "\n",
        "* Firstly, it totally depends on the type of dataset we are dealing with.\n",
        "\n",
        "* We could remove the the null values by dropping it using pandas.\n",
        "* dropna() function can be used to drop the null values in the dataframe.\n",
        "* If the dataset consists of numbers, such as continious numbers, we can find the <b>mean</b> and fill the missing values."
      ],
      "metadata": {
        "id": "aTCMQ0ORfsjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design a pipeline for building a classification model. Include steps for data preprocessing.\n",
        "Classification Model Pipeline\n",
        "1. Data Collection <br>\n",
        "Gather the dataset from relevant sources (e.g., CSV files, databases, APIs).\n",
        "2. Data Exploration <br>\n",
        "Understand the Data: Examine the dataset to understand its structure, features, and target variable.\n",
        "\n",
        "Visualize Data - Use plots to identify patterns, distributions, and potential outliers.\n",
        "\n",
        "3. Data Preprocessing\n",
        "Handle Missing Values: <br>\n",
        "Remove rows with missing values or fill them using imputation methods (mean, median, mode).\n",
        "\n",
        "Feature Scaling: <br>\n",
        "Scale numerical features using techniques like Min-Max scaling or Standardization (Z-score normalization).\n",
        "\n",
        "Encode Categorical Variables: <br>\n",
        "Convert categorical variables into numerical format using methods like one-hot encoding or label encoding.\n",
        "\n",
        "Feature Selection: <br>\n",
        "Identify and select relevant features that contribute significantly to the model's performance.\n",
        "\n",
        "4. Split the Data <br>\n",
        "Divide the dataset into training and testing sets (commonly 70-80% training and 20-30% testing).\n",
        "\n",
        "5. Model Selection <br>\n",
        "Choose a classification algorithm (e.g., Logistic Regression, Decision Trees, Random Forest, Support Vector Machine).\n",
        "\n",
        "6. Model Training <br>\n",
        "Train the selected model using the training dataset.\n",
        "\n",
        "7. Model Evaluation <br>\n",
        "Evaluate the model's performance on the test set using metrics like accuracy, precision, recall, F1-score, and confusion matrix.\n",
        "8. Hyperparameter Tuning <br>\n",
        "Optimize model performance by tuning hyperparameters using techniques like Grid Search or Random Search.\n",
        "9. Final Model Training <br>\n",
        "Retrain the model on the entire dataset using the best hyperparameters obtained from tuning.\n",
        "10. Deployment <br>\n",
        "Deploy the trained model for predictions in a production environment."
      ],
      "metadata": {
        "id": "waJuaSFMglwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Coding <h1>\n"
      ],
      "metadata": {
        "id": "Xv6qnqHyhrmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python script to implement a decision tree classifier using Scikit-learn.\n"
      ],
      "metadata": {
        "id": "X-Ve99-bhvPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python script to implement a decision tree classifier using Scikit-learn.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "data = {'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "        'feature2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
        "        'target': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Initialize and train the Decision Tree classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR3e2quaicfs",
        "outputId": "c3fa19f9-6d5a-4cda-8d5c-582ea1632097"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 2\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "n_0grvqzizcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Case Study</h1>\n",
        "<b> A company wants to predict employee attrition. What kind of ML problem is this? Which algorithms would you choose and why? </b>"
      ],
      "metadata": {
        "id": "4RbAEqJQi8rD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's first figure out what is attrition ?\n",
        "* Attrition means finding out if an employee has left the company or not.\n",
        "* Left the company or not becomes the <b> Target Variable </b>\n",
        "* Thus, the type of problem is classification Problem.\n",
        "\n",
        "I Would choose the following algorithms to evalute the F1 score and accuracy of the model for our classification problem:\n",
        "\n",
        "* <b> Logistic Regression </b>\n",
        "\n",
        "* Why: It is simple, easy to implement, and interpret. It provides probabilities for each class, which can help in understanding the likelihood of attrition.\n",
        "\n",
        "* <b> Random Forest </b>\n",
        "\n",
        "* Why: An ensemble method that improves the accuracy by combining multiple decision trees. It reduces the risk of overfitting and increases robustness."
      ],
      "metadata": {
        "id": "iKk38XhFjH6z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5baCeLNti_Sv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}